{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dfd1a41-09ef-4361-b89c-56df6acafa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ONEAPI_INSTALL=/opt/intel/oneapi\n"
     ]
    }
   ],
   "source": [
    "# ignore all warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import torch\n",
    "\n",
    "%env ONEAPI_INSTALL=/opt/intel/oneapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817ff712-78cd-4814-91b6-775ff053d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.apis import inference_model\n",
    "\n",
    "def get_average_inference_time(model, data):\n",
    "    \"\"\"\n",
    "    does a model warm up and times the model runtime\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # warm up\n",
    "        for _ in range(10):\n",
    "            inference_model(model, data)\n",
    "\n",
    "        # measure\n",
    "        import time\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            output = inference_model(model, data)\n",
    "        end = time.time()\n",
    "        average_inference_time = (end-start)/10*1000\n",
    "    \n",
    "    return average_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db85bc57-9c71-407b-bc3f-c8ae412d61bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ckpt/stdc2_512x1024_80k_cityscapes_20220222_132015-fb1e3a1a.pth\n",
      "average time: 587.69 ms\n"
     ]
    }
   ],
   "source": [
    "from mmseg.apis import inference_model, init_model, show_result_pyplot\n",
    "import mmcv\n",
    "\n",
    "config_file = 'configs/stdc/stdc2_4xb12-80k_cityscapes-512x1024.py'\n",
    "checkpoint_file = 'ckpt/stdc2_512x1024_80k_cityscapes_20220222_132015-fb1e3a1a.pth'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_model(config_file, checkpoint_file, device='cpu')\n",
    "\n",
    "# test a single image and show the results\n",
    "img = 'demo/demo.png'  # or img = mmcv.imread(img), which will only load it once\n",
    "# result = inference_model(model, img)\n",
    "infer_time = get_average_inference_time(model, img)\n",
    "print('average time: %.2f ms' % infer_time)\n",
    "\n",
    "# visualize the results in a new window\n",
    "# show_result_pyplot(model, img, result, show=True)\n",
    "# or save the visualization results to image files\n",
    "# you can change the opacity of the painted segmentation map in (0, 1].\n",
    "# show_result_pyplot(model, img, result, show=False, out_file='result.jpg', opacity=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef75ad7-35b0-4398-b46e-bd8a64d74715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: ckpt/stdc2_512x1024_80k_cityscapes_20220222_132015-fb1e3a1a.pth\n",
      "average time: 410.16 ms\n"
     ]
    }
   ],
   "source": [
    "from mmseg.apis import inference_model, init_model, show_result_pyplot\n",
    "import mmcv\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "T = 800 # number of inference samples\n",
    "\n",
    "config_file = 'configs/stdc/stdc2_4xb12-80k_cityscapes-512x1024.py'\n",
    "checkpoint_file = 'ckpt/stdc2_512x1024_80k_cityscapes_20220222_132015-fb1e3a1a.pth'\n",
    "\n",
    "imgs = []\n",
    "\n",
    "filepath = './data/cityscapes/leftImg8bit/test'\n",
    "extends = '.png'\n",
    "filelist=[]\n",
    "for root, dirnames, filenames in os.walk(filepath):\n",
    "    for filename in filenames:\n",
    "        if extends in filename:\n",
    "            filelist.append(os.path.join(root,filename))\n",
    "\n",
    "imgs_path = np.random.choice(filelist, T, replace=False)\n",
    "for img_path in imgs_path:\n",
    "    imgs.append(mmcv.imread(img_path))\n",
    "\n",
    "    \n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_model(config_file, checkpoint_file, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    # warm up\n",
    "    for img in imgs[:20]:\n",
    "        inference_model(model, img)\n",
    "\n",
    "    # measure\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for img in imgs:\n",
    "        output = inference_model(model, img)\n",
    "    end = time.time()\n",
    "    average_inference_time = (end-start)/T*1000\n",
    "\n",
    "print('average time: %.2f ms' % average_inference_time)\n",
    "    \n",
    "# # visualize inference result\n",
    "# ## random a palette with size 256x3\n",
    "# palette = np.random.randint(0, 256, size=(256, 3))\n",
    "# color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
    "# for label, color in enumerate(palette):\n",
    "#     color_seg[seg == label, :] = color\n",
    "# # convert to BGR\n",
    "# color_seg = color_seg[..., ::-1]\n",
    "# img = img * 0.5 + color_seg * 0.5\n",
    "# img = img.astype(np.uint8)\n",
    "# cv2.imwrite('output_segmentation.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131995e2-1e0a-47bc-b756-aecda67363a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "import neural_compressor # Intel® Neural Compressor is an open-source Python library that supports popular model compression techniques\n",
    "#######################################################################################################################################\n",
    "\n",
    "# check if Neural Compressor is above v2.0\n",
    "assert float(neural_compressor.__version__) >= 2.0, \"The below APIs work with Intel® Neural Compressor 2.0 and above\" \n",
    "\n",
    "from neural_compressor.quantization import fit\n",
    "from neural_compressor.config import PostTrainingQuantConfig, TuningCriterion, AccuracyCriterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de266ee5-a502-455b-977f-71da2119f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIou(object):\n",
    "    def __init__(self, numClass=19):\n",
    "        self.numClass = numClass\n",
    "        self.confusionMatrix = torch.zeros((self.numClass,) * 2)  # 混淆矩阵（空）\n",
    "\n",
    "        \n",
    "    def update(self, imgPredict, imgLabel):\n",
    "        assert imgPredict.shape == imgLabel.shape\n",
    "        self.confusionMatrix += self.genConfusionMatrix(imgPredict, imgLabel)  # 得到混淆矩阵\n",
    "        return self.confusionMatrix\n",
    "\n",
    "\n",
    "    def genConfusionMatrix(self, imgPredict, imgLabel, ignore_labels=[]):  #\n",
    "        # remove classes from unlabeled pixels in gt image and predict\n",
    "        mask = (imgLabel >= 0) & (imgLabel < self.numClass)\n",
    "        for IgLabel in ignore_labels:\n",
    "            mask &= (imgLabel != IgLabel)\n",
    "        label = self.numClass * imgLabel[mask] + imgPredict[mask]\n",
    "        count = torch.bincount(label, minlength=self.numClass ** 2)\n",
    "        confusionMatrix = count.view(self.numClass, self.numClass)\n",
    "        # print(confusionMatrix)\n",
    "        return confusionMatrix\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # clear preds and labels storage\n",
    "        self.confusionMatrix = torch.zeros((self.numClass, self.numClass))\n",
    "\n",
    "        \n",
    "    def IntersectionOverUnion(self):\n",
    "        # Intersection = TP Union = TP + FP + FN\n",
    "        # IoU = TP / (TP + FP + FN)\n",
    "        intersection = torch.diag(self.confusionMatrix)  # 取对角元素的值，返回列表\n",
    "        union = torch.sum(self.confusionMatrix, axis=1) + torch.sum(self.confusionMatrix, axis=0) - torch.diag(\n",
    "            self.confusionMatrix)  # axis = 1表示混淆矩阵行的值，返回列表； axis = 0表示取混淆矩阵列的值，返回列表\n",
    "        IoU = intersection / union  # 返回列表，其值为各个类别的IoU\n",
    "        return IoU\n",
    "        \n",
    "        \n",
    "    def result(self):\n",
    "        IoU = self.IntersectionOverUnion()\n",
    "        mIoU = IoU[IoU<float('inf')].mean()# 求各类别IoU的平均\n",
    "        return mIoU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d02ee02-9d60-41ff-b4cd-4dae840117ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 23:48:20 [INFO] Start auto tuning.\n",
      "2023-10-07 23:48:20 [INFO] Create evaluation function according to evaluation dataloader and metric                and Execute the tuning process.\n",
      "2023-10-07 23:48:20 [INFO] Adaptor has 4 recipes.\n",
      "2023-10-07 23:48:20 [INFO] 0 recipes specified by user.\n",
      "2023-10-07 23:48:20 [INFO] 3 recipes require future tuning.\n",
      "2023-10-07 23:48:20 [INFO] *** Initialize auto tuning\n",
      "2023-10-07 23:48:20 [INFO] {\n",
      "2023-10-07 23:48:20 [INFO]     'PostTrainingQuantConfig': {\n",
      "2023-10-07 23:48:20 [INFO]         'AccuracyCriterion': {\n",
      "2023-10-07 23:48:20 [INFO]             'criterion': 'relative',\n",
      "2023-10-07 23:48:20 [INFO]             'higher_is_better': True,\n",
      "2023-10-07 23:48:20 [INFO]             'tolerable_loss': 0.01,\n",
      "2023-10-07 23:48:20 [INFO]             'absolute': None,\n",
      "2023-10-07 23:48:20 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7facf4b89df0>>,\n",
      "2023-10-07 23:48:20 [INFO]             'relative': 0.01\n",
      "2023-10-07 23:48:20 [INFO]         },\n",
      "2023-10-07 23:48:20 [INFO]         'approach': 'post_training_static_quant',\n",
      "2023-10-07 23:48:20 [INFO]         'backend': 'default',\n",
      "2023-10-07 23:48:20 [INFO]         'calibration_sampling_size': [\n",
      "2023-10-07 23:48:20 [INFO]             100\n",
      "2023-10-07 23:48:20 [INFO]         ],\n",
      "2023-10-07 23:48:20 [INFO]         'device': 'cpu',\n",
      "2023-10-07 23:48:20 [INFO]         'diagnosis': False,\n",
      "2023-10-07 23:48:20 [INFO]         'domain': 'auto',\n",
      "2023-10-07 23:48:20 [INFO]         'example_inputs': None,\n",
      "2023-10-07 23:48:20 [INFO]         'excluded_precisions': [\n",
      "2023-10-07 23:48:20 [INFO]         ],\n",
      "2023-10-07 23:48:20 [INFO]         'framework': 'pytorch_fx',\n",
      "2023-10-07 23:48:20 [INFO]         'inputs': [\n",
      "2023-10-07 23:48:20 [INFO]         ],\n",
      "2023-10-07 23:48:20 [INFO]         'model_name': '',\n",
      "2023-10-07 23:48:20 [INFO]         'op_name_dict': None,\n",
      "2023-10-07 23:48:20 [INFO]         'op_type_dict': None,\n",
      "2023-10-07 23:48:20 [INFO]         'outputs': [\n",
      "2023-10-07 23:48:20 [INFO]         ],\n",
      "2023-10-07 23:48:20 [INFO]         'quant_format': 'default',\n",
      "2023-10-07 23:48:20 [INFO]         'quant_level': 'auto',\n",
      "2023-10-07 23:48:20 [INFO]         'recipes': {\n",
      "2023-10-07 23:48:20 [INFO]             'smooth_quant': False,\n",
      "2023-10-07 23:48:20 [INFO]             'smooth_quant_args': {\n",
      "2023-10-07 23:48:20 [INFO]             },\n",
      "2023-10-07 23:48:20 [INFO]             'fast_bias_correction': False,\n",
      "2023-10-07 23:48:20 [INFO]             'weight_correction': False,\n",
      "2023-10-07 23:48:20 [INFO]             'gemm_to_matmul': True,\n",
      "2023-10-07 23:48:20 [INFO]             'graph_optimization_level': None,\n",
      "2023-10-07 23:48:20 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2023-10-07 23:48:20 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2023-10-07 23:48:20 [INFO]             'pre_post_process_quantization': True,\n",
      "2023-10-07 23:48:20 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2023-10-07 23:48:20 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2023-10-07 23:48:20 [INFO]             ],\n",
      "2023-10-07 23:48:20 [INFO]             'dedicated_qdq_pair': False\n",
      "2023-10-07 23:48:20 [INFO]         },\n",
      "2023-10-07 23:48:20 [INFO]         'reduce_range': None,\n",
      "2023-10-07 23:48:20 [INFO]         'TuningCriterion': {\n",
      "2023-10-07 23:48:20 [INFO]             'max_trials': 100,\n",
      "2023-10-07 23:48:20 [INFO]             'objective': 'performance',\n",
      "2023-10-07 23:48:20 [INFO]             'strategy': 'basic',\n",
      "2023-10-07 23:48:20 [INFO]             'strategy_kwargs': None,\n",
      "2023-10-07 23:48:20 [INFO]             'timeout': 0\n",
      "2023-10-07 23:48:20 [INFO]         },\n",
      "2023-10-07 23:48:20 [INFO]         'use_bf16': True\n",
      "2023-10-07 23:48:20 [INFO]     }\n",
      "2023-10-07 23:48:20 [INFO] }\n",
      "2023-10-07 23:48:21 [INFO] Attention Blocks: 0\n",
      "2023-10-07 23:48:21 [INFO] FFN Blocks: 0\n",
      "2023-10-07 23:48:21 [INFO] Pass query framework capability elapsed time: 600.45 ms\n",
      "2023-10-07 23:48:21 [INFO] Get FP32 model baseline.\n",
      "2023-10-07 23:48:22 [WARNING] The dataloader didn't include label, will try input without label!\n",
      "2023-10-07 23:48:23 [ERROR] Unexpected exception TypeError(\"default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\") happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/adaptor/pytorch.py\", line 911, in eval_func\n",
      "    for idx, (input, label) in enumerate(dataloader):\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n",
      "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
      "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/quantization.py\", line 215, in fit\n",
      "    strategy.traverse()\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/strategy/auto.py\", line 108, in traverse\n",
      "    super().traverse()\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/strategy/strategy.py\", line 405, in traverse\n",
      "    self._prepare_tuning()\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/strategy/strategy.py\", line 323, in _prepare_tuning\n",
      "    self._eval_baseline()\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/strategy/strategy.py\", line 907, in _eval_baseline\n",
      "    self.baseline = self._evaluate(self.model)\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/strategy/strategy.py\", line 1441, in _evaluate\n",
      "    val = self.objectives.evaluate(eval_func, model)\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/objective.py\", line 397, in evaluate\n",
      "    acc = eval_func(model)\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/utils/create_obj_from_config.py\", line 162, in eval_func\n",
      "    return adaptor.evaluate(model, dataloader, postprocess,\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/adaptor/pytorch.py\", line 3699, in evaluate\n",
      "    return self.model_eval(model_, dataloader, postprocess, metrics, measurer, iteration)\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/adaptor/pytorch.py\", line 1018, in model_eval\n",
      "    results = self.eval_func(model, dataloader, postprocess, metrics, measurer,\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/neural_compressor/adaptor/pytorch.py\", line 944, in eval_func\n",
      "    for idx, input in enumerate(dataloader):\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/home/u204644/.conda/envs/mm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n",
      "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
      "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
      "2023-10-07 23:48:23 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n"
     ]
    }
   ],
   "source": [
    "from neural_compressor.data import DataLoader, Datasets\n",
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "# from mmseg.datasets import CityscapesDataset\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torch.utils.data import DataLoader\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "# tuning_criterion = TuningCriterion(max_trials=5)\n",
    "# accuracy_criterion = AccuracyCriterion(tolerable_loss=0.1)\n",
    "# conf = PostTrainingQuantConfig(approach=\"dynamic\", tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion)\n",
    "\n",
    "# conf = PostTrainingQuantConfig()\n",
    "# dataset = Datasets(\"pytorch\")[\"dummy\"](shape=(1, 512, 1024, 3))\n",
    "# dataloader = DataLoader(framework=\"pytorch\", dataset=dataset)\n",
    "\n",
    "miou = MIou()\n",
    "config = PostTrainingQuantConfig()\n",
    "\n",
    "dataset = Cityscapes('./data/cityscapes', split='test', mode='fine',\n",
    "                     target_type='semantic', transforms=)\n",
    "test_dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "q_model = fit(model, config, calib_dataloader=test_dataloader, eval_dataloader=test_dataloader, eval_metric=miou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "712cf796-13ca-4b02-8a4c-dd71bda6e7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average time: 533.78 ms\n"
     ]
    }
   ],
   "source": [
    "# test a single image and show the results\n",
    "img = 'demo/demo.png'  # or img = mmcv.imread(img), which will only load it once\n",
    "# result = inference_model(model, img)\n",
    "infer_time = get_average_inference_time(q_model._model, img)\n",
    "print('average time: %.2f ms' % infer_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1789c7a7-6f2d-4b0d-9bfa-9c7740ba5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(q_model._model.state_dict(), 'ckpt/stdc2_512x1024_80k_cityscapes_20220222_132015-fb1e3a1a-quan.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5caab2b-fef7-4fc8-a14a-16a7f2c0881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadd741-f20b-40d8-ab18-8e59d8c24b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_compressor as inc\n",
    "print(\"neural_compressor version {}\".format(inc.__version__))\n",
    "\n",
    "import torch\n",
    "print(\"torch {}\".format(torch.__version__))\n",
    "\n",
    "from neural_compressor.quantization import fit\n",
    "from neural_compressor.config import PostTrainingQuantConfig, AccuracyCriterion, TuningCriterion, Options\n",
    "from neural_compressor.data import DataLoader\n",
    "from torchvision.datasets import Cityscapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b357e0-39be-45c9-ad52-6caab33c6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "fp32_model_file = 'ckpt/stdc2_512x1024_80k_cityscapes_20220222_132015-fb1e3a1a.pth'\n",
    "int8_model = 'ckpt/stdc2_512x1024_80k_cityscapes_20220222_132015-fb1e3a1a-quan-static.pth'\n",
    "\n",
    "config_file = 'configs/stdc/stdc2_4xb12-80k_cityscapes-512x1024.py'\n",
    "checkpoint_file = 'ckpt/stdc2_512x1024_80k_cityscapes_20220222_132015-fb1e3a1a.pth'\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_model(config_file, checkpoint_file, device='cpu')\n",
    "\n",
    "\n",
    "if compare_ver(inc.__version__, \"2.0\")>=0:\n",
    "    print(f\"Compatible Intel Neural Compressor version detected : v{inc.__version__} \")\n",
    "else:\n",
    "    raise Exception(f\"Installed Intel Neural Compressor version[v{inc.__version__}] is NOT compatible. Please upgrade to version 2.0 or higher.\")\n",
    "\n",
    "# train_dataset, test_dataset = fashion_mnist.download_dataset()\n",
    "# calib_dataloader = DataLoader(framework='pytorch', dataset=train_dataset, batch_size = batch_size)\n",
    "# eval_dataloader = DataLoader(framework='pytorch', dataset=test_dataset, batch_size = batch_size)\n",
    "\n",
    "train_dataset = Cityscapes('./data/cityscapes', split='train', mode='fine', target_type='semantic')\n",
    "test_dataset = Cityscapes('./data/cityscapes', split='test', mode='fine', target_type='semantic')\n",
    "calib_dataloader = DataLoader(framework='pytorch', dataset=train_dataset, batch_size = batch_size)\n",
    "eval_dataloader = DataLoader(framework='pytorch', dataset=test_dataset, batch_size = batch_size)\n",
    "\n",
    "tuning_criterion = TuningCriterion(max_trials=100)\n",
    "option = Options(workspace='./nc_workspace')\n",
    "config = PostTrainingQuantConfig(approach=\"static\", tuning_criterion=tuning_criterion,\n",
    "accuracy_criterion = AccuracyCriterion(\n",
    "  higher_is_better=True, \n",
    "  criterion='relative',  \n",
    "  tolerable_loss=0.01  \n",
    "  )\n",
    ")\n",
    "q_model = fit(\n",
    "    model=model,\n",
    "    conf=config,\n",
    "    calib_dataloader=calib_dataloader,\n",
    "    eval_dataloader=eval_dataloader\n",
    "    )\n",
    "    \n",
    "q_model = auto_tune(fp32_model_file, batch_size)\n",
    "q_model.save(int8_model)\n",
    "print(\"Save int8 model to {}\".format(int8_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "mm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
